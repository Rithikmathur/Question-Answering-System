{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85757c6c",
   "metadata": {},
   "source": [
    "We have questions data and sentences data, now we'll take one-one question from questions file and we'll convert them into their semantic representation and at the same time we'll have to convert the answers from sentences data into their semantic representations. And then we have find out for a given question what are the k closest answers from these answers. K could be any numerical value maximum till numbers of sentences, usually we take k as 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32f71eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0588c012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# \"distilbert-base-uncased\" means we have used pretrained base varient model and tokenizer of DistilBERT with all the words normalized.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6b48b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0b157d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_handle = open(\"sentences.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9efce40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_data = json.load(file_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a790347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A pandemic is an epidemic of an infectious disease that has spread across a large region, for instance multiple continents or worldwide, affecting a substantial number of people.',\n",
       " 'The most fatal pandemic in recorded history was the Black Death (also known as The Plague), which killed an estimated 75â€“200 million people in the 14th century.',\n",
       " 'Current pandemics include COVID-19 (SARS-CoV-2) and HIV/AIDS.',\n",
       " 'As of 2018, approximately 37.9 million people are infected with HIV globally.',\n",
       " 'Cholera is an infection of the small intestine by some strains of the bacterium Vibrio cholerae.',\n",
       " 'Classic cholera symptom is large amounts of watery diarrhea that lasts a few days. Vomiting and muscle cramps may also occur. Diarrhea can be so severe that it leads within hours to severe dehydration and electrolyte imbalance.',\n",
       " 'The COVID-19 pandemic, also known as the coronavirus pandemic, is an ongoing pandemic of coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2).',\n",
       " 'Common symptoms of COVID-19 include fever, cough, fatigue, breathing difficulties, and loss of smell.',\n",
       " 'The Plague of Cyprian was a pandemic that afflicted the Roman Empire about from AD 249 to 262.',\n",
       " 'The Spanish flu, also known as the 1918 flu pandemic, was an unusually deadly influenza pandemic caused by the H1N1 influenza A virus.',\n",
       " 'The death toll of Spanish Flu is estimated to have been somewhere between 17 million and 50 million, and possibly as high as 100 million, making it one of the deadliest pandemics in human history.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad704743",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41334d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# document could be an answer or a question.\n",
    "# This function will take one document at a time tokenize it and then detach and squeeze it.\n",
    "def encoder(document):\n",
    "    \n",
    "    return model(**tokenizer(document,return_tensors='pt'))[0].detach().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96813790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This list will contain the matrix of every document.\n",
    "documents_embeddings = []\n",
    "\n",
    "for document in sentences_data:\n",
    "    \n",
    "    documents_embeddings.append(encoder(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cf3868e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 768])\n",
      "torch.Size([37, 768])\n",
      "torch.Size([25, 768])\n",
      "torch.Size([18, 768])\n",
      "torch.Size([24, 768])\n",
      "torch.Size([55, 768])\n",
      "torch.Size([57, 768])\n",
      "torch.Size([24, 768])\n",
      "torch.Size([27, 768])\n",
      "torch.Size([35, 768])\n",
      "torch.Size([43, 768])\n"
     ]
    }
   ],
   "source": [
    "for embedding in documents_embeddings:\n",
    "    print(embedding.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aacf2db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba1e7de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrices for us are of no use we need then to be as vectors, so we'll take the average of each matrix and convert it into the vector of 768 dimensions.\n",
    "docs2vecs = []\n",
    "\n",
    "for embedding in documents_embeddings:\n",
    "    docs2vecs.append(torch.mean(embedding,dim=0)) # DistilBERT is written in pytorch library and the encoded representations return from DistilBERT are in Pytorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7d9c26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a40713ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Blank Index, as Facebook AI Semantic Search library requires data to be stored in index structure.\n",
    "index = faiss.IndexIDMap(faiss.IndexFlatIP(768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b780f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.array(range(0, len(sentences_data)))\n",
    "ids = np.asarray(ids.astype('int64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b7f8748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have put the documents into different rows in index structure. \n",
    "# As we ran a loop documents(semantic representation(Pytorch tensors)) convert it into numpy array put them into a list and\n",
    "# converted the complete list into numpy array ad then we saved our matrix into index data structure.\n",
    "index.add_with_ids((np.array([doc_vec.numpy() for doc_vec in docs2vecs])),ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccafbc94",
   "metadata": {},
   "source": [
    "We don't need to write K Nearest Algorithm as Index Data Structure will give us the answers with that format. That's why we used index data structure for our answers, so that it will become easy for us to K Nearest Neighbour algorithm on answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ae47d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will accept a query and return the k closest answers belongs to that query.\n",
    "def search(query,k):\n",
    "    \n",
    "    encoded_query = encoder(query) # It will give the matrix of 768 dimensions\n",
    "    \n",
    "    encoded_query = torch.mean(encoded_query,dim=0) # It converted the 768 dimensional matrix to a 768 dimentional vector.\n",
    "    \n",
    "    encoded_query = encoded_query.unsqueeze(dim=0).numpy() # It will reshape it to (1,768) convert the vector into array.\n",
    "    \n",
    "    k_nn_docs = index.search(encoded_query,k) # Index have a search function which will returns the k nearest neighbour answer to the query.\n",
    "    \n",
    "    semantic_similarity_scores = k_nn_docs[0][0] # It calculates the semantic similarity score between query and answer.\n",
    "    # Higher the score stronger the similarity, Lower the score weaker the similarity.\n",
    "    \n",
    "    results = [sentences_data[idx] for idx in k_nn_docs[1][0]] # resuts will have the sentence for the query asked.\n",
    "    \n",
    "    return list(zip(results,semantic_similarity_scores)),k_nn_docs # It zipped the result with semantic similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10c1d159",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_file_handle = open(\"questions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d6b9a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_data = json.load(query_file_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "718c380e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How many people have died during Black Death?',\n",
       " 'Which diseases can be transmitted by animals?',\n",
       " 'Connection between climate change and a likelihood of a pandemic',\n",
       " 'What is an example of a latent virus',\n",
       " 'Viruses in nanotechnology',\n",
       " 'Giant viruses classification',\n",
       " 'What are the notable pandemic prevention organizations?',\n",
       " 'How many leprosy outbreaks are known to happen?',\n",
       " 'What are the geographic areas with the highest transmission of malaria?',\n",
       " 'How to prevent the spread of viral infections?']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e8a279",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df9d6117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connection between climate change and a likelihood of a pandemic'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63fb7b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A pandemic is an epidemic of an infectious disease that has spread across a large region, for instance multiple continents or worldwide, affecting a substantial number of people.',\n",
       "  60.540653)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(query_data[2],1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
